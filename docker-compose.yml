# Docker Compose configuration for CPU-only environments
# Use this file if you don't have a GPU or if you encounter GPU-related issues

services:
  # Ollama service: Provides the LLM capability (CPU-only mode)
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama  # Persist model data across restarts
    ports:
      - "11434:11434"  # Expose Ollama API port
    restart: unless-stopped
    networks:
      - rag-network

  # Main application service: Streamlit UI and RAG pipeline
  webapp:
    build: .
    volumes:
      - ./app.py:/app/app.py
      - ./init-ollama.sh:/app/init-ollama.sh
      - ./data:/app/data  # Map local data directory directly to container
      - ./documents:/app/documents  # Map local documents directory directly to container
    ports:
      - "8501:8501"  # Expose Streamlit web interface
    restart: unless-stopped
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - ollama
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        chmod +x /app/init-ollama.sh
        ./init-ollama.sh
        echo "===================================================="
        echo "  RAG Pipeline is starting..."
        echo "  Once initialization is complete, access the app at:"
        echo "  http://localhost:8501"
        echo "===================================================="
        python -m streamlit run app.py --server.address=0.0.0.0 --server.headless=true
        echo "Application is running at http://localhost:8501"
    networks:
      - rag-network

# Persistent volumes definition
volumes:
  ollama_data:  # Stores downloaded models

# Network configuration
networks:
  rag-network:
    driver: bridge  # Standard bridge network for container communication 